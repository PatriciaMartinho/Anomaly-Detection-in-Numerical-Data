{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Data Generator </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Polytechnic University of Leiria </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Patrícia Isabel Santos Martinho </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from benfordslaw import benfordslaw # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import entropy  \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=1000 # features\n",
    "n=2000 # instances\n",
    "tB=0.3 # proportion of anomalous rows\n",
    "tm=0.1 # anomalies in an anomalous row\n",
    "intensity_uniforme=1 # intensity (uniform noise)\n",
    "intensity_gauss=10 # intensity (gaussian noise)\n",
    "prop_outliers=0.05\n",
    "outliers_band=(100000, 500000) # band of outliers\n",
    "np.random.seed(1666)\n",
    "alpha=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qty_tB=int(n*tB) # absolute amount of anomalous rows\n",
    "print(qty_tB)\n",
    "qt_BL=n-qty_tB # absolute amount of BL conform rows\n",
    "print(qt_BL)\n",
    "A_count=int(m*tm) # absolute amount of anomalies in an anomalous row \n",
    "BL_count=int(m*(1-tm)) # absolute amount of BL conform numbers in an anomalous row "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion to generator BL conform numbers\n",
    "def ben_numbers_generation (n):\n",
    "    uniforme=np.random.uniform(low=0, high=1, size=n)\n",
    "    bfl_numb=10**uniforme\n",
    "    return bfl_numb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the first digit of a number\n",
    "def get_first_digit(number):\n",
    "    num = abs(number)  # Work only with positive values\n",
    "    if num == 0:\n",
    "        return 0\n",
    "    while num < 1:  # If it is a small decimal number, multiply until it has a digit in the whole part\n",
    "        num *= 10\n",
    "    while num >= 10:  # If it is a large number, divide until on only one digit\n",
    "        num //= 10\n",
    "    return int(num)  # Returns the first digit as an integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the frequencies of the first digits\n",
    "def first_digit_frequency(numbers):\n",
    "    frequencies = np.zeros(9)  # To store the frequencies of digits 1 through 9\n",
    "    for number in numbers:\n",
    "        first_digit = get_first_digit(number)\n",
    "        frequencies[first_digit - 1] += 1  # Increases the digit count\n",
    "    return frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the distribution expected by Benford’s Law\n",
    "def BL_distribution(n):\n",
    "    distribution = np.log10(1 + 1 / np.arange(1, 10))  # Benford’s law for digits 1 to 9\n",
    "    distribution[-1] = 1 - sum(distribution[:-1])  # Adjusts the last probability\n",
    "    return distribution*n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accumulated frequencies from absolute frequencies\n",
    "\n",
    "def Fr (fr):\n",
    "    Frs=[]\n",
    "    Frs.append(fr[0])\n",
    "    for i in range(1,len(fr)):\n",
    "        Frs.append(fr[i]+Frs[i-1])\n",
    "    return Frs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Gaussian noise\n",
    "\n",
    "def sum_gaussian_noise(d, intensity):\n",
    "    \"\"\"Adds Gaussian noise to the data\"\"\"\n",
    "    noise = np.random.normal(0, intensity, len(d))\n",
    "    return d + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform noise\n",
    "\n",
    "def sum_uniforme_noise(d, intensity):\n",
    "    \"\"\"Adds uniform noise to the data\"\"\"\n",
    "    noise = np.random.uniform(-intensity, intensity, len(d))\n",
    "    return d + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed noise (gaussin noise + uniform noise)\n",
    "\n",
    "def sum_mix_noise(d, intensity_gauss, intensity_uniforme):\n",
    "    \"\"\"Adds a mixture of Gaussian and uniform noise to the data\"\"\"\n",
    "    noise_gauss = np.random.normal(0, intensity_gauss, len(d))\n",
    "    noise_uniforme = np.random.uniform(-intensity_uniforme, intensity_uniforme, len(d))\n",
    "    return d + noise_gauss + noise_uniforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers\n",
    "\n",
    "def sum_outliers(d, proportion, outliers_band):\n",
    "    \"\"\"Adds moderate or extreme outliers to the data set\"\"\"\n",
    "    n_outliers = int(len(d) * proportion)\n",
    "    indices = np.random.choice(range(len(d)), n_outliers, replace=False) # replace=falss means without repeating\n",
    "    for i in indices:\n",
    "        d[i] = np.random.uniform(*outliers_band)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute mean deviation\n",
    "\n",
    "def calculate_mad(observ, expected):\n",
    "    return np.mean(np.abs(observ - expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorov-smirnov\n",
    "\n",
    "def calculate_ks (observ, expected):\n",
    "\n",
    "    # Calculate accumulated frequencies\n",
    "    obs_acum=np.array(Fr(observ))\n",
    "    esp_acum=np.array(Fr(expected))\n",
    "    \n",
    "    # Calculate distance\n",
    "    return np.max(np.abs(obs_acum - esp_acum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "\n",
    "def calculate_euclidian (observ, expected):\n",
    "    return np.sqrt(np.sum((observ - expected) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hellinger distance\n",
    "\n",
    "def calculate_hellinger(observ, expected):\n",
    "    return np.sqrt(0.5 * np.sum((np.sqrt(observ) - np.sqrt(expected))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kullback-Leiber divergence\n",
    "\n",
    "def calculate_kl(observ, expected):\n",
    "    # Calculate KL Divergence\n",
    "    kl_value = entropy(observ, expected)  # scipy.stats.entropy calculates KL when we pass two distributions\n",
    "    \n",
    "    return kl_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis tests\n",
    "\n",
    "def t_hip (test, observations, number_simulations):\n",
    "        # Expected frequencies for Benford’s Law\n",
    "    expected = BL_distribution(number_simulations)\n",
    "    expected_norm=expected/sum(expected)\n",
    "\n",
    "    # Generate samples that follow Benford’s Law\n",
    "    simulated_values = []\n",
    "\n",
    "    for _ in range(number_simulations):\n",
    "        simulated = np.random.choice(np.arange(1, 10), p=np.array(expected_norm), size=int(sum(observations)))\n",
    "        simulated_freq = [np.sum(simulated == d) for d in range(1, 10)]\n",
    "        match test:\n",
    "            case \"mad\":\n",
    "                mad_simulado = calculate_mad(simulated_freq, expected)\n",
    "                simulated_values.append(mad_simulado)\n",
    "            case \"ks\":\n",
    "                ks_simulado= calculate_ks(simulated_freq,expected)\n",
    "                simulated_values.append(ks_simulado)\n",
    "            case \"euc\":\n",
    "                euclidiana_simulado= calculate_euclidian(simulated_freq,expected)\n",
    "                simulated_values.append(euclidiana_simulado)\n",
    "            case \"hel\":\n",
    "                hellinger_simulado= calculate_hellinger(simulated_freq,expected)\n",
    "                simulated_values.append(hellinger_simulado)\n",
    "            case \"kl\":\n",
    "                kl_simulado= calculate_kl(simulated_freq,expected)\n",
    "                simulated_values.append(kl_simulado)\n",
    "\n",
    "        \n",
    "\n",
    "     # calculate observed value:\n",
    "    match test:\n",
    "        case \"mad\":\n",
    "            obs_value = calculate_mad(observations, expected)\n",
    "        case \"ks\":\n",
    "            obs_value= calculate_ks(observations,expected)\n",
    "        case \"euc\":\n",
    "            obs_value = calculate_euclidian(observations, expected)\n",
    "        case \"hel\":\n",
    "            obs_value = calculate_hellinger(observations, expected)\n",
    "        case \"kl\":\n",
    "            obs_value = calculate_kl(observations, expected)\n",
    "        \n",
    "\n",
    "    # Calculate p-value\n",
    "    p_value_test = np.mean(np.array(simulated_values) >= obs_value) # proportion of values higher than expected.\n",
    "\n",
    "    return(obs_value,p_value_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Fisher(p_values):\n",
    "\n",
    "    fisher_stat = -2 * sum(np.log(p_values))\n",
    "    if fisher_stat == float('-inf'):\n",
    "        print(p_values)\n",
    "    combined_p_value = 1 - chi2.cdf(fisher_stat, 2 * len(p_values)) # chi2.cdf -> cumulative chi2 distribution\n",
    "    return combined_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I did this function to solve situations of incomplete confusion matrices and avoid errors\n",
    "\n",
    "def conf_matrix(actual_class, previsto):\n",
    "    # Generate the confusion matrix\n",
    "    labels = [0, 1]  # Expected classes\n",
    "    conf_matrix = confusion_matrix(actual_class, previsto, labels=labels)\n",
    "\n",
    "    tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "    # Direct extraction of values if both classes exist\n",
    "    if conf_matrix.shape == (2, 2):\n",
    "        tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    else:  # If there is only one class in the data\n",
    "        if 0 in actual_class:\n",
    "            tn = conf_matrix[0, 0] if conf_matrix.shape[0] > 0 else 0\n",
    "            fp = conf_matrix[0, 1] if conf_matrix.shape[1] > 1 else 0\n",
    "        if 1 in actual_class:\n",
    "            fn = conf_matrix[1, 0] if conf_matrix.shape[0] > 1 else 0\n",
    "            tp = conf_matrix[1, 1] if conf_matrix.shape[1] > 1 else 0\n",
    "    \n",
    "    return tn, fp, fn, tp, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw chart of the proportions of the digits\n",
    "\n",
    "def digit_chart (d,nl):\n",
    "    # Creation of the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Data for the Benford’s Law chart\n",
    "    prop_ds = np.array(d[nl,:-1])\n",
    "         \n",
    "    # Calculate the frequency of the first digits\n",
    "    xv = range(1, 10)\n",
    "    yv = BL_distribution(m)\n",
    "    yv = yv/m\n",
    "\n",
    "    # Plot the proportions according to Benford’s Law as a line\n",
    "    ax.plot(xv, yv, marker='o', label=\"Benford's Law Distribution\", linestyle='-', color='blue', alpha=0.6)\n",
    "\n",
    "    # Notes for the values of Benford’s Law\n",
    "    for i, value in enumerate(yv):\n",
    "        ax.annotate(f'{value:.3f}',  # Round the annotation to 3 decimal places\n",
    "                    xy=(xv[i], value),\n",
    "                    xytext=(0, 5),  # Offset at the annotation position\n",
    "                    textcoords='offset points',\n",
    "                    ha='center',\n",
    "                    va='bottom')\n",
    "\n",
    "    # Data for the dataset chart\n",
    " \n",
    "    x = list(range(1, 10)) \n",
    "    y = first_digit_frequency(prop_ds)\n",
    "    y = y/len(prop_ds)\n",
    "\n",
    "    # Plot the proportions of the dataset as a line\n",
    "    ax.plot(x, y, marker='o', label=f\"Digits frequency in line nr. {nl}\", linestyle='-', color='orange', alpha=0.6)\n",
    "\n",
    "    # Annotations for the dataset values\n",
    "    for i, value in enumerate(y):\n",
    "        ax.annotate(f'{value}',\n",
    "                    xy=(x[i], value),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords='offset points',\n",
    "                    ha='center',\n",
    "                    va='bottom')\n",
    "\n",
    "    # Axis and subtitle settings\n",
    "    ax.set_xlabel(\"First Digit\", fontsize=14)\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=14)\n",
    "    ax.legend(fontsize=14)\n",
    "    ax.set_xticks(list(range(1, 10)))  # Defining x axis ticks\n",
    "    ax.set_xticklabels(list(range(1, 10)))  # Labels for the ticks\n",
    "\n",
    "    # display the graph\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniforme=np.random.uniform(low=0, high=1, size=m)\n",
    "bfl_numb=10**uniforme \n",
    "result=np.append(bfl_numb,0) # follows the law of Benford, label 0\n",
    "print(len(result))\n",
    "print(\"----------------------\")\n",
    "\n",
    "i=1\n",
    "i2=qt_BL+1\n",
    "while i<qt_BL:\n",
    "    bfl_numb=ben_numbers_generation (m)\n",
    "    bfl_numb=np.append(bfl_numb,0) # 0 --> no anomalies\n",
    "    bfl_numb=abs(bfl_numb)\n",
    "    result = np.vstack([result, bfl_numb])\n",
    "    i=i+1\n",
    "\n",
    "while i2 in range(qt_BL+1,n+1):\n",
    "    ben_aux=ben_numbers_generation (A_count)\n",
    "    #anom = sum_gaussian_noise(ben_aux, intensity_gauss)\n",
    "    #anom = sum_uniforme_noise(ben_aux, intensity_uniforme)\n",
    "    #anom = sum_mix_noise(ben_aux, intensity_gauss, intensity_uniforme)\n",
    "    anom = sum_outliers(ben_aux, prop_outliers, outliers_band)\n",
    "    anom=np.random.uniform(low=0, high=100000, size=A_count)\n",
    "    anom=abs(anom)\n",
    "    ben=np.random.uniform(low=0, high=1, size=BL_count)\n",
    "    bfl_numb=10**ben\n",
    "    bfl_numb=abs(bfl_numb)\n",
    "    bfl_numb=np.append(bfl_numb,anom)\n",
    "    bfl_numb=np.append(bfl_numb,1) # 1 --> with anomalies\n",
    "    result = np.vstack([result, bfl_numb])\n",
    "    i2=i2+1\n",
    "df_result=pd.DataFrame(result)\n",
    "df_final = df_result.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "numpy_df=array = df_final.to_numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_class = numpy_df[:, -1]\n",
    "actual_class=actual_class.astype(int)\n",
    "print(actual_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(actual_class)\n",
    "print(\"Negativos\", counts[0])\n",
    "print(\"Positivos\", counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_chart (numpy_df,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type (numpy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Dataset.txt', numpy_df, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
