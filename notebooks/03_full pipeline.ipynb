{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Full Project </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Polytechnic University of Leiria </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Patrícia Isabel Santos Martinho </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from benfordslaw import benfordslaw # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from scipy.stats import chisquare\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from scipy.stats import chi2, entropy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=1000 # features\n",
    "n=2000 # instances\n",
    "tB=0.3 # proportion of anomalous rows\n",
    "tm=0.1 # anomalies in an anomalous row\n",
    "intensity_uniforme=1 # intensity (uniform noise)\n",
    "intensity_gauss=10 # intensity (gaussian noise)\n",
    "prop_outliers=0.05\n",
    "outliers_band=(100000, 500000) # band of outliers\n",
    "np.random.seed(1666)\n",
    "alpha=0.05\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qty_tB=int(n*tB) # absolute amount of anomalous rows\n",
    "print(qty_tB)\n",
    "qt_BL=n-qty_tB # absolute amount of BL conform rows\n",
    "print(qt_BL)\n",
    "A_count=int(m*tm) # absolute amount of anomalies in an anomalous row \n",
    "BL_count=int(m*(1-tm)) # absolute amount of BL conform numbers in an anomalous row "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion to generator BL conform numbers\n",
    "def ben_numbers_generation (n):\n",
    "    uniforme=np.random.uniform(low=0, high=1, size=n)\n",
    "    bfl_numb=10**uniforme\n",
    "    return bfl_numb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the first digit of a number\n",
    "def get_first_digit(number):\n",
    "    num = abs(number)  # Work only with positive values\n",
    "    if num == 0:\n",
    "        return 0\n",
    "    while num < 1:  # If it is a small decimal number, multiply until it has a digit in the whole part\n",
    "        num *= 10\n",
    "    while num >= 10:  # If it is a large number, divide until on only one digit\n",
    "        num //= 10\n",
    "    return int(num)  # Returns the first digit as an integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the first digits\n",
    "def first_digit_array (numbers):\n",
    "    firts_digits=[]\n",
    "    for number in numbers:\n",
    "        firts_digits.append(get_first_digit(number))\n",
    "    firts_digits = [x for x in firts_digits if x != 0]\n",
    "    return firts_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the frequencies of the first digits\n",
    "def first_digit_frequency(numbers):\n",
    "    frequencies = np.zeros(9)  # To store the frequencies of digits 1 through 9\n",
    "    for number in numbers:\n",
    "        first_digit = get_first_digit(number)\n",
    "        frequencies[first_digit - 1] += 1  # Increases the digit count\n",
    "    return frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the distribution expected by Benford’s Law\n",
    "def BL_distribution(n):\n",
    "    distribution = np.log10(1 + 1 / np.arange(1, 10))  # Benford’s law for digits 1 to 9\n",
    "    distribution[-1] = 1 - sum(distribution[:-1])  # Adjusts the last probability\n",
    "    return distribution*n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accumulated frequencies from absolute frequencies\n",
    "\n",
    "def Fr (fr):\n",
    "    Frs=[]\n",
    "    Frs.append(fr[0])\n",
    "    for i in range(1,len(fr)):\n",
    "        Frs.append(fr[i]+Frs[i-1])\n",
    "    return Frs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Gaussian noise\n",
    "\n",
    "def sum_gaussian_noise(d, intensity):\n",
    "    \"\"\"Adds Gaussian noise to the data\"\"\"\n",
    "    noise = np.random.normal(0, intensity, len(d))\n",
    "    return d + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform noise\n",
    "\n",
    "def sum_uniforme_noise(d, intensity):\n",
    "    \"\"\"Adds uniform noise to the data\"\"\"\n",
    "    noise = np.random.uniform(-intensity, intensity, len(d))\n",
    "    return d + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed noise (gaussin noise + uniform noise)\n",
    "\n",
    "def sum_mix_noise(d, intensity_gauss, intensity_uniforme):\n",
    "    \"\"\"Adds a mixture of Gaussian and uniform noise to the data\"\"\"\n",
    "    noise_gauss = np.random.normal(0, intensity_gauss, len(d))\n",
    "    noise_uniforme = np.random.uniform(-intensity_uniforme, intensity_uniforme, len(d))\n",
    "    return d + noise_gauss + noise_uniforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers\n",
    "\n",
    "def sum_outliers(d, proportion, outliers_band):\n",
    "    \"\"\"Adds moderate or extreme outliers to the data set\"\"\"\n",
    "    n_outliers = int(len(d) * proportion)\n",
    "    indices = np.random.choice(range(len(d)), n_outliers, replace=False) # replace=falss means without repeating\n",
    "    for i in indices:\n",
    "        d[i] = np.random.uniform(*outliers_band)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute mean deviation\n",
    "\n",
    "def calculate_mad(observ, expected):\n",
    "    return np.mean(np.abs(observ - expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorov-smirnov\n",
    "\n",
    "def calculate_ks (observ, expected):\n",
    "\n",
    "    # Calculate accumulated frequencies\n",
    "    obs_acum=np.array(Fr(observ))\n",
    "    esp_acum=np.array(Fr(expected))\n",
    "    \n",
    "    # Calculate distance\n",
    "    return np.max(np.abs(obs_acum - esp_acum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "\n",
    "def calculate_euclidian (observ, expected):\n",
    "    return np.sqrt(np.sum((observ - expected) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hellinger distance\n",
    "\n",
    "def calculate_hellinger(observ, expected):\n",
    "    return np.sqrt(0.5 * np.sum((np.sqrt(observ) - np.sqrt(expected))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kullback-Leiber divergence\n",
    "\n",
    "def calculate_kl(observ, expected):\n",
    "    # Calculate KL Divergence\n",
    "    kl_value = entropy(observ, expected)  # scipy.stats.entropy calculates KL when we pass two distributions\n",
    "    \n",
    "    return kl_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis tests\n",
    "\n",
    "def t_hip (test, observations, number_simulations):\n",
    "        # Expected frequencies for Benford’s Law\n",
    "    expected = BL_distribution(number_simulations)\n",
    "    expected_norm=expected/sum(expected)\n",
    "\n",
    "    # Generate samples that follow Benford’s Law\n",
    "    simulated_values = []\n",
    "\n",
    "    for _ in range(number_simulations):\n",
    "        simulated = np.random.choice(np.arange(1, 10), p=np.array(expected_norm), size=int(sum(observations)))\n",
    "        simulated_freq = [np.sum(simulated == d) for d in range(1, 10)]\n",
    "        match test:\n",
    "            case \"mad\":\n",
    "                simulated_mad = calculate_mad(simulated_freq, expected)\n",
    "                simulated_values.append(simulated_mad)\n",
    "            case \"ks\":\n",
    "                simulated_ks= calculate_ks(simulated_freq,expected)\n",
    "                simulated_values.append(simulated_ks)\n",
    "            case \"euc\":\n",
    "                simulated_eucl= calculate_euclidian(simulated_freq,expected)\n",
    "                simulated_values.append(simulated_eucl)\n",
    "            case \"hel\":\n",
    "                simulated_hell= calculate_hellinger(simulated_freq,expected)\n",
    "                simulated_values.append(simulated_hell)\n",
    "            case \"kl\":\n",
    "                simulated_kl= calculate_kl(simulated_freq,expected)\n",
    "                simulated_values.append(simulated_kl)\n",
    "\n",
    "        \n",
    "\n",
    "     # calculate observed value:\n",
    "    match test:\n",
    "        case \"mad\":\n",
    "            obs_value = calculate_mad(observations, expected)\n",
    "        case \"ks\":\n",
    "            obs_value= calculate_ks(observations,expected)\n",
    "        case \"euc\":\n",
    "            obs_value = calculate_euclidian(observations, expected)\n",
    "        case \"hel\":\n",
    "            obs_value = calculate_hellinger(observations, expected)\n",
    "        case \"kl\":\n",
    "            obs_value = calculate_kl(observations, expected)\n",
    "        \n",
    "\n",
    "    # Calculate p-value\n",
    "    p_value_test = np.mean(np.array(simulated_values) >= obs_value) # proportion of values higher than expected.\n",
    "\n",
    "    return(obs_value,p_value_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Fisher(p_values):\n",
    "\n",
    "    fisher_stat = -2 * sum(np.log(p_values))\n",
    "    if fisher_stat == float('-inf'):\n",
    "        print(p_values)\n",
    "    combined_p_value = 1 - chi2.cdf(fisher_stat, 2 * len(p_values)) # chi2.cdf -> cumulative chi2 distribution\n",
    "    return combined_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I did this function to solve situations of incomplete confusion matrices and avoid errors\n",
    "\n",
    "def conf_matrix(actual_class, predicted):\n",
    "    # Generate the confusion matrix\n",
    "    labels = [0, 1]  # Expected classes\n",
    "    conf_matrix = confusion_matrix(actual_class, predicted, labels=labels)\n",
    "\n",
    "    tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "    # Direct extraction of values if both classes exist\n",
    "    if conf_matrix.shape == (2, 2):\n",
    "        tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    else:  # If there is only one class in the data\n",
    "        if 0 in actual_class:\n",
    "            tn = conf_matrix[0, 0] if conf_matrix.shape[0] > 0 else 0\n",
    "            fp = conf_matrix[0, 1] if conf_matrix.shape[1] > 1 else 0\n",
    "        if 1 in actual_class:\n",
    "            fn = conf_matrix[1, 0] if conf_matrix.shape[0] > 1 else 0\n",
    "            tp = conf_matrix[1, 1] if conf_matrix.shape[1] > 1 else 0\n",
    "    \n",
    "    return tn, fp, fn, tp, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_roc_curve(actual_class, p_values, alpha_values):\n",
    "    fprs = []\n",
    "    tprs = []\n",
    "\n",
    "    p_values = p_values.flatten()  # Ensure that p_values is a one-dimensional vector\n",
    "\n",
    "    # Calculate overall ROC and AUC before loop (to avoid unnecessary calculations)\n",
    "    fpr_full, tpr_full, _ = roc_curve(actual_class, p_values)\n",
    "    auc_value = auc(fpr_full, tpr_full)\n",
    "    print(f\"AUC total: {auc_value}\")\n",
    "\n",
    "    for alpha in alpha_values:  \n",
    "        predicted = (p_values < alpha).astype(int) # positive values - with anomalies\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(actual_class, predicted).ravel()\n",
    "\n",
    "        # Calculate the rate of false positives and true positives\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  \n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  \n",
    "\n",
    "        fprs.append(fpr)\n",
    "        tprs.append(tpr)\n",
    "\n",
    "    # Generate the ROC curve\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(fprs, tprs, marker=\"o\", label=\"ROC curve\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Reference line (random model)\n",
    "    plt.xlabel(\"1-Specificity (FPR)\")\n",
    "    plt.ylabel(\"Sensibility (TPR)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return np.array(fprs), np.array(tprs), np.array(alpha_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw chart of the proportions of the digits\n",
    "\n",
    "def digit_chart (d,nl):\n",
    "    # Creation of the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Data for the Benford’s Law chart\n",
    "    prop_ds = np.array(d[nl,:-1])\n",
    "         \n",
    "    # Calculate the frequency of the first digits\n",
    "    xv = range(1, 10)\n",
    "    yv = BL_distribution(m)\n",
    "    yv = yv/m\n",
    "\n",
    "    # Plot the proportions according to Benford’s Law as a line\n",
    "    ax.plot(xv, yv, marker='o', label=\"Benford's Law Distribution\", linestyle='-', color='blue', alpha=0.6)\n",
    "\n",
    "    # Notes for the values of Benford’s Law\n",
    "    for i, value in enumerate(yv):\n",
    "        ax.annotate(f'{value:.3f}',  # Round the annotation to 3 decimal places\n",
    "                    xy=(xv[i], value),\n",
    "                    xytext=(0, 5),  # Offset at the annotation position\n",
    "                    textcoords='offset points',\n",
    "                    ha='center',\n",
    "                    va='bottom')\n",
    "\n",
    "    # Data for the dataset chart\n",
    " \n",
    "    x = list(range(1, 10)) \n",
    "    y = first_digit_frequency(prop_ds)\n",
    "    y = y/len(prop_ds)\n",
    "\n",
    "    # Plot the proportions of the dataset as a line\n",
    "    ax.plot(x, y, marker='o', label=f\"Digits frequency in line nr. {nl}\", linestyle='-', color='orange', alpha=0.6)\n",
    "\n",
    "    # Annotations for the dataset values\n",
    "    for i, value in enumerate(y):\n",
    "        ax.annotate(f'{value}',\n",
    "                    xy=(x[i], value),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords='offset points',\n",
    "                    ha='center',\n",
    "                    va='bottom')\n",
    "\n",
    "    # Axis and subtitle settings\n",
    "    ax.set_xlabel(\"First Digit\", fontsize=14)\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=14)\n",
    "    ax.legend(fontsize=14)\n",
    "    ax.set_xticks(list(range(1, 10)))  # Defining x axis ticks\n",
    "    ax.set_xticklabels(list(range(1, 10)))  # Labels for the ticks\n",
    "\n",
    "    # display the graph\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniforme=np.random.uniform(low=0, high=1, size=m)\n",
    "bfl_numb=10**uniforme \n",
    "result=np.append(bfl_numb,0) # follows the law of Benford, label 0\n",
    "print(len(result))\n",
    "print(\"----------------------\")\n",
    "\n",
    "i=1\n",
    "i2=qt_BL+1\n",
    "while i<qt_BL:\n",
    "    bfl_numb=ben_numbers_generation (m)\n",
    "    bfl_numb=np.append(bfl_numb,0) # 0 --> no anomalies\n",
    "    bfl_numb=abs(bfl_numb)\n",
    "    result = np.vstack([result, bfl_numb])\n",
    "    i=i+1\n",
    "\n",
    "while i2 in range(qt_BL+1,n+1):\n",
    "    ben_aux=ben_numbers_generation (A_count)\n",
    "    #anom = sum_gaussian_noise(ben_aux, intensity_gauss)\n",
    "    #anom = sum_uniforme_noise(ben_aux, intensity_uniforme)\n",
    "    #anom = sum_mix_noise(ben_aux, intensity_gauss, intensity_uniforme)\n",
    "    anom = sum_outliers(ben_aux, prop_outliers, outliers_band)\n",
    "    anom=np.random.uniform(low=0, high=100000, size=A_count)\n",
    "    anom=abs(anom)\n",
    "    ben=np.random.uniform(low=0, high=1, size=BL_count)\n",
    "    bfl_numb=10**ben\n",
    "    bfl_numb=abs(bfl_numb)\n",
    "    bfl_numb=np.append(bfl_numb,anom)\n",
    "    bfl_numb=np.append(bfl_numb,1) # 1 --> with anomalies\n",
    "    result = np.vstack([result, bfl_numb])\n",
    "    i2=i2+1\n",
    "df_result=pd.DataFrame(result)\n",
    "df_final = df_result.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "numpy_df=array = df_final.to_numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_class = numpy_df[:, -1]\n",
    "actual_class=actual_class.astype(int)\n",
    "print(actual_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(actual_class)\n",
    "print(\"Negatives\", counts[0])\n",
    "print(\"Positives\", counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_chart (numpy_df,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Dataset.txt', numpy_df, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify compliance with the Benford's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_distribution = BL_distribution(m)\n",
    "\n",
    "predicted_chi2=[]\n",
    "p_values_chi2=[]\n",
    "\n",
    "predicted_f = []\n",
    "predicted_ks_1samp=[]\n",
    "predicted_ks=[]\n",
    "predicted_MAD=[]\n",
    "predicted_hel=[]\n",
    "predicted_euc=[]\n",
    "predicted_kl=[]\n",
    "p_values_ks_1samp=[]\n",
    "p_values_ks=[]\n",
    "p_values_MAD=[]\n",
    "p_values_euc=[]\n",
    "p_values_hel=[]\n",
    "p_values_kl=[]\n",
    "\n",
    "\n",
    "for n_row in range(n):\n",
    "    dfbl = np.array(numpy_df[n_row,:-1])\n",
    "    p_values=[]\n",
    "\n",
    "    first_digits = first_digit_array (dfbl)\n",
    "        \n",
    "    first_digit_frequencies = first_digit_frequency(dfbl)\n",
    "    \n",
    "    # Apply the chi-square test\n",
    "    chi2_stat, p_value_chi2 = chisquare(first_digit_frequencies, expected_distribution)\n",
    "    p_value_chi2=1e-15 if p_value_chi2 <(1e-15) else p_value_chi2\n",
    "    predicted_label_chi2 = 1 if p_value_chi2 < alpha else 0  \n",
    "    p_values.append(p_value_chi2)\n",
    "    p_values_chi2.append(p_value_chi2)\n",
    "    predicted_chi2.append(predicted_label_chi2)\n",
    "  \n",
    "    # Apply the absolute mean deviation\n",
    "    MAD_stat, p_value_MAD=t_hip(\"mad\",first_digit_frequencies,m)\n",
    "    p_value_MAD=1e-15 if p_value_MAD <(1e-15) else p_value_MAD\n",
    "    predicted_label_MAD = 1 if p_value_MAD < alpha else 0\n",
    "    p_values.append(p_value_MAD)\n",
    "    p_values_MAD.append(p_value_MAD)\n",
    "    predicted_MAD.append(predicted_label_MAD)\n",
    "    \n",
    "\n",
    "    # Apply to the distance from Kolmogorov-smirnov \n",
    "    ks_stat, p_value_ks=t_hip(\"ks\",first_digit_frequencies,m)\n",
    "    p_value_ks=1e-15 if p_value_ks <(1e-15) else p_value_ks\n",
    "    predicted_label_ks = 1 if p_value_ks < alpha else 0\n",
    "    p_values.append(p_value_ks)\n",
    "    p_values_ks.append(p_value_ks)\n",
    "    predicted_ks.append(predicted_label_ks)\n",
    "\n",
    "    # Apply the euclidean distance\n",
    "    euc_stat, p_value_euc=t_hip(\"euc\",first_digit_frequencies,m)\n",
    "    p_value_euc=1e-15 if p_value_euc <(1e-15) else p_value_euc\n",
    "    predicted_label_euc = 1 if p_value_euc < alpha else 0\n",
    "    p_values.append(p_value_euc)\n",
    "    p_values_euc.append(p_value_euc)\n",
    "    predicted_euc.append(predicted_label_euc)\n",
    "\n",
    "     # Apply the distance of Hellinger\n",
    "    hel_stat, p_value_hel=t_hip(\"hel\",first_digit_frequencies,m)\n",
    "    p_value_hel=1e-15 if p_value_hel <(1e-15) else p_value_hel\n",
    "    predicted_label_hel = 1 if p_value_hel < alpha else 0\n",
    "    p_values.append(p_value_hel)\n",
    "    p_values_hel.append(p_value_hel)\n",
    "    predicted_hel.append(predicted_label_hel)\n",
    "\n",
    "    # Apply the divergence of Kulback-Leibler\n",
    "    kl_stat, p_value_kl=t_hip(\"kl\",first_digit_frequencies,m)\n",
    "    p_value_kl=1e-15 if p_value_kl <(1e-15) else p_value_kl\n",
    "    predicted_label_kl = 1 if p_value_kl < alpha else 0\n",
    "    p_values.append(p_value_kl)\n",
    "    p_values_kl.append(p_value_kl)\n",
    "    predicted_kl.append(predicted_label_kl)\n",
    "\n",
    "\n",
    "    # Fisher’s combination\n",
    "    p_value_fisher = Fisher(p_values) \n",
    "    predicted_label=1 if p_value_fisher<alpha else 0\n",
    "    predicted_f.append(predicted_label)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of metrics\n",
    "metricas = [\"Chi-square\", \"mean absolute deviation\",\"Kolmogorov-Smirnov\", \"Euclidean\", \"Hellinger\", \"Kullback-Leibler\", \"Fisher\"] \n",
    "tns, fps, fns, tps = [], [], [], []\n",
    "\n",
    "\n",
    "predicteds = [predicted_chi2, predicted_MAD, predicted_ks, predicted_euc,predicted_hel, predicted_kl, predicted_f]\n",
    "\n",
    "for forecast in predicteds:\n",
    "    tn, fp, fn, tp, mc = conf_matrix(actual_class, forecast) \n",
    "    tns.append(tn)\n",
    "    fps.append(fp)\n",
    "    fns.append(fn)\n",
    "    tps.append(tp)\n",
    "\n",
    "# Create DataFrame Pandas\n",
    "df = pd.DataFrame({\n",
    "    \"Metric\": metricas,\n",
    "    \"TN\": tns,\n",
    "    \"FP\": fps,\n",
    "    \"FN\": fns,\n",
    "    \"TP\": tps\n",
    "})\n",
    "\n",
    "# Add evaluation metrics\n",
    "df[\"Precision\"] = df[\"TP\"] / (df[\"TP\"] + df[\"FP\"])\n",
    "df[\"Recall\"] = df[\"TP\"] / (df[\"TP\"] + df[\"FN\"])\n",
    "df[\"F1-score\"] = 2 * (df[\"Precision\"] * df[\"Recall\"]) / (df[\"Precision\"] + df[\"Recall\"])\n",
    "\n",
    "# Display the table\n",
    "print(\"Dataset:\")\n",
    "print(f\"Number of features:{m}\")\n",
    "print(f\"Number of instances:{n}\")\n",
    "print(\"  \")\n",
    "print(\"Actual class:\")\n",
    "print(f\"Positives --> with anomalies: {counts[1]}\")\n",
    "print(f\"Negatives --> no anomalies: {counts[0]}\")\n",
    "print(\"  \")\n",
    "print(f\"Proportion of anomalies in an anomalous row: {tm}\")\n",
    "print(\"-----------\")\n",
    "print(f\"|Alpha={alpha}|\")\n",
    "print(\"-----------\")\n",
    "print(df)\n",
    "\n",
    "# Save results in Excel format\n",
    "df.to_excel(f\"Performance.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_values = p_values_chi2\n",
    "alpha_values = np.arange(0, 1, 0.001) \n",
    "p_values = np.array([p_values])\n",
    "\n",
    "# statistics\n",
    "print(\"Statistics of p_values:\")\n",
    "print(\"Average:\", np.mean(p_values))\n",
    "print(\"Standard Deviation:\", np.std(p_values))\n",
    "print(\"Min:\", np.min(p_values))\n",
    "print(\"Max:\", np.max(p_values))\n",
    "\n",
    "fprs, tprs, alpha_values = generate_roc_curve(actual_class, p_values, alpha_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprs = np.array(fprs)\n",
    "tprs = np.array(tprs)\n",
    "alpha_values = np.array(alpha_values)\n",
    "\n",
    "# Criterion of Youden\n",
    "youden_index = np.argmax(tprs - fprs)  \n",
    "best_alpha_youden = alpha_values[youden_index]\n",
    "\n",
    "# Point closest to (0,1)\n",
    "distances = np.sqrt((1 - tprs)**2 + fprs**2)\n",
    "best_alpha_distance = alpha_values[np.argmin(distances)]\n",
    "\n",
    "print(f\"Best cut-off point by Youden’s criterion: {best_alpha_youden}\")\n",
    "print(f\"Best cut-off point by the criterion of least distance: {best_alpha_distance}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
