{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Classification Model </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Polytechnic University of Leiria </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Patrícia Isabel Santos Martinho </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from benfordslaw import benfordslaw # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from scipy.stats import chisquare\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import entropy  # Função para calcular KL\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas=1000 # features\n",
    "linhas=2000 # instances\n",
    "prop_linhas_manipuladas=0.3 # proportion of anomalous rows\n",
    "prop_manipul_linha=0.1 # anomalies in an anomalous row\n",
    "alpha=0.05\n",
    "df_ficheiro=\"Dataset.txt\" # datset file name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas_manipuladas=int(linhas*prop_linhas_manipuladas) # absolute amount of anomalous rows\n",
    "print(linhas_manipuladas)\n",
    "linhas_s_fraude=linhas-linhas_manipuladas # absolute amount of BL conform rows\n",
    "print(linhas_s_fraude)\n",
    "qtde_fraude_linhaF=int(colunas*prop_manipul_linha) # absolute amount of anomalies in an anomalous row \n",
    "qtde_ben_linhaF=int(colunas*(1-prop_manipul_linha)) # absolute amount of BL conform numbers in an anomalous row "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the distribution expected by Benford’s Law\n",
    "def distribuicao_benford(n):\n",
    "    distribution = np.log10(1 + 1 / np.arange(1, 10))  # Benford’s law for digits 1 to 9\n",
    "    distribution[-1] = 1 - sum(distribution[:-1])  # Adjusts the last probability\n",
    "    return distribution*n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the first digit of a number\n",
    "def obter_primeiro_digito(numero):\n",
    "    num = abs(numero)  # Work only with positive values\n",
    "    if num == 0:\n",
    "        return 0\n",
    "    while num < 1:  # If it is a small decimal number, multiply until it has a digit in the whole part\n",
    "        num *= 10\n",
    "    while num >= 10:  # If it is a large number, divide until on only one digit\n",
    "        num //= 10\n",
    "    return int(num)  # Returns the first digit as an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the first digits\n",
    "def array_primeiros_digitos (numeros):\n",
    "    primeiros_digitos=[]\n",
    "    for numero in numeros:\n",
    "        primeiros_digitos.append(obter_primeiro_digito(numero))\n",
    "    primeiros_digitos = [x for x in primeiros_digitos if x != 0]\n",
    "    return primeiros_digitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the frequencies of the first digits\n",
    "def frequencia_primeiros_digitos(numeros):\n",
    "    frequencias = np.zeros(9)  # To store the frequencies of digits 1 through 9\n",
    "    for numero in numeros:\n",
    "        primeiro_digito = obter_primeiro_digito(numero)\n",
    "        frequencias[primeiro_digito - 1] += 1  # Increases the digit count\n",
    "    return frequencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accumulated frequencies from absolute frequencies\n",
    "\n",
    "def f_acumulada (f_relativas):\n",
    "    f_acumuladas=[]\n",
    "    f_acumuladas.append(f_relativas[0])\n",
    "    for i in range(1,len(f_relativas)):\n",
    "        f_acumuladas.append(f_relativas[i]+f_acumuladas[i-1])\n",
    "    return f_acumuladas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute mean deviation\n",
    "\n",
    "def calcular_mad(observadas, esperadas):\n",
    "    return np.mean(np.abs(observadas - esperadas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorov-smirnov\n",
    "\n",
    "def calcular_ks (observadas, esperadas):\n",
    "\n",
    "    # Calculate accumulated frequencies\n",
    "    obs_acum=np.array(f_acumulada(observadas))\n",
    "    esp_acum=np.array(f_acumulada(esperadas))\n",
    "    \n",
    "    # Calculate distance\n",
    "    return np.max(np.abs(obs_acum - esp_acum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "\n",
    "def calcular_euclidiana (observadas, esperadas):\n",
    "    return np.sqrt(np.sum((observadas - esperadas) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hellinger distance\n",
    "\n",
    "def calcular_hellinger(observadas, esperadas):\n",
    "    return np.sqrt(0.5 * np.sum((np.sqrt(observadas) - np.sqrt(esperadas))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kullback-Leiber divergence\n",
    "\n",
    "def calcular_kl(observadas, esperadas):\n",
    "    # Calculate KL Divergence\n",
    "    kl_value = entropy(observadas, esperadas)  # scipy.stats.entropy calculates KL when we pass two distributions\n",
    "    \n",
    "    return kl_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis tests\n",
    "\n",
    "def teste_hipoteses (teste, observacoes, numero_simulacoes):\n",
    "        # Expected frequencies for Benford’s Law\n",
    "    esperadas = distribuicao_benford(numero_simulacoes)\n",
    "    esperadas_norm=esperadas/sum(esperadas)\n",
    "\n",
    "    # Generate samples that follow Benford’s Law\n",
    "    valores_simulados = []\n",
    "\n",
    "    for _ in range(numero_simulacoes):\n",
    "        simulados = np.random.choice(np.arange(1, 10), p=np.array(esperadas_norm), size=int(sum(observacoes)))\n",
    "        freq_simuladas = [np.sum(simulados == d) for d in range(1, 10)]\n",
    "        match teste:\n",
    "            case \"mad\":\n",
    "                mad_simulado = calcular_mad(freq_simuladas, esperadas)\n",
    "                valores_simulados.append(mad_simulado)\n",
    "            case \"ks\":\n",
    "                ks_simulado= calcular_ks(freq_simuladas,esperadas)\n",
    "                valores_simulados.append(ks_simulado)\n",
    "            case \"euc\":\n",
    "                euclidiana_simulado= calcular_euclidiana(freq_simuladas,esperadas)\n",
    "                valores_simulados.append(euclidiana_simulado)\n",
    "            case \"hel\":\n",
    "                hellinger_simulado= calcular_hellinger(freq_simuladas,esperadas)\n",
    "                valores_simulados.append(hellinger_simulado)\n",
    "            case \"kl\":\n",
    "                kl_simulado= calcular_kl(freq_simuladas,esperadas)\n",
    "                valores_simulados.append(kl_simulado)\n",
    "\n",
    "        \n",
    "\n",
    "     # calculate observed value:\n",
    "    match teste:\n",
    "        case \"mad\":\n",
    "            valor_observado = calcular_mad(observacoes, esperadas)\n",
    "        case \"ks\":\n",
    "            valor_observado= calcular_ks(observacoes,esperadas)\n",
    "        case \"euc\":\n",
    "            valor_observado = calcular_euclidiana(observacoes, esperadas)\n",
    "        case \"hel\":\n",
    "            valor_observado = calcular_hellinger(observacoes, esperadas)\n",
    "        case \"kl\":\n",
    "            valor_observado = calcular_kl(observacoes, esperadas)\n",
    "        \n",
    "\n",
    "    # Calculate p-value\n",
    "    p_value_teste = np.mean(np.array(valores_simulados) >= valor_observado) # proportion of values higher than expected.\n",
    "\n",
    "    return(valor_observado,p_value_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Fisher(p_values):\n",
    "\n",
    "    fisher_stat = -2 * sum(np.log(p_values))\n",
    "    if fisher_stat == float('-inf'):\n",
    "        print(p_values)\n",
    "    combined_p_value = 1 - chi2.cdf(fisher_stat, 2 * len(p_values)) # chi2.cdf -> cumulative chi2 distribution\n",
    "    return combined_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I did this function to solve situations of incomplete confusion matrices and avoid errors\n",
    "\n",
    "def matriz_confusao(class_real, previsto):\n",
    "    # Generate the confusion matrix\n",
    "    labels = [0, 1]  # Expected classes\n",
    "    conf_matrix = confusion_matrix(class_real, previsto, labels=labels)\n",
    "\n",
    "    tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "    # Direct extraction of values if both classes exist\n",
    "    if conf_matrix.shape == (2, 2):\n",
    "        tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    else:  # If there is only one class in the data\n",
    "        if 0 in class_real:\n",
    "            tn = conf_matrix[0, 0] if conf_matrix.shape[0] > 0 else 0\n",
    "            fp = conf_matrix[0, 1] if conf_matrix.shape[1] > 1 else 0\n",
    "        if 1 in class_real:\n",
    "            fn = conf_matrix[1, 0] if conf_matrix.shape[0] > 1 else 0\n",
    "            tp = conf_matrix[1, 1] if conf_matrix.shape[1] > 1 else 0\n",
    "    \n",
    "    return tn, fp, fn, tp, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_roc(class_real, p_values, alpha_values):\n",
    "    fprs = []\n",
    "    tprs = []\n",
    "\n",
    "    p_values = p_values.flatten()  # Ensure that p_values is a one-dimensional vector\n",
    "\n",
    "    # Calculate overall ROC and AUC before loop (to avoid unnecessary calculations)\n",
    "    fpr_full, tpr_full, _ = roc_curve(class_real, p_values)\n",
    "    auc_value = auc(fpr_full, tpr_full)\n",
    "    print(f\"AUC total: {auc_value}\")\n",
    "\n",
    "    for alpha in alpha_values:  \n",
    "        previsto = (p_values < alpha).astype(int) # positive values - with anomalies\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(class_real, previsto).ravel()\n",
    "\n",
    "        # Calculate the rate of false positives and true positives\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  \n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  \n",
    "\n",
    "        fprs.append(fpr)\n",
    "        tprs.append(tpr)\n",
    "\n",
    "    # Generate the ROC curve\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(fprs, tprs, marker=\"o\", label=\"ROC curve\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Reference line (random model)\n",
    "    plt.xlabel(\"1-Specificity (FPR)\")\n",
    "    plt.ylabel(\"Sensibility (TPR)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return np.array(fprs), np.array(tprs), np.array(alpha_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw chart of the proportions of the digits\n",
    "\n",
    "def grafico_digitos (dados,nl):\n",
    "    # Creation of the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Data for the Benford’s Law chart\n",
    "    prop_ds = np.array(dados[nl,:-1])\n",
    "         \n",
    "    # Calculate the frequency of the first digits\n",
    "    xv = range(1, 10)\n",
    "    yv = distribuicao_benford(colunas)\n",
    "    yv = yv/colunas\n",
    "\n",
    "    # Plot the proportions according to Benford’s Law as a line\n",
    "    ax.plot(xv, yv, marker='o', label=\"Benford's Law Distribution\", linestyle='-', color='blue', alpha=0.6)\n",
    "\n",
    "    # Notes for the values of Benford’s Law\n",
    "    for i, value in enumerate(yv):\n",
    "        ax.annotate(f'{value:.3f}',  # Round the annotation to 3 decimal places\n",
    "                    xy=(xv[i], value),\n",
    "                    xytext=(0, 5),  # Offset at the annotation position\n",
    "                    textcoords='offset points',\n",
    "                    ha='center',\n",
    "                    va='bottom')\n",
    "\n",
    "    # Data for the dataset chart\n",
    " \n",
    "    x = list(range(1, 10)) \n",
    "    y = frequencia_primeiros_digitos(prop_ds)\n",
    "    y = y/len(prop_ds)\n",
    "\n",
    "    # Plot the proportions of the dataset as a line\n",
    "    ax.plot(x, y, marker='o', label=f\"Digits frequency in line nr. {nl}\", linestyle='-', color='orange', alpha=0.6)\n",
    "\n",
    "    # Annotations for the dataset values\n",
    "    for i, value in enumerate(y):\n",
    "        ax.annotate(f'{value}',\n",
    "                    xy=(x[i], value),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords='offset points',\n",
    "                    ha='center',\n",
    "                    va='bottom')\n",
    "\n",
    "    # Axis and subtitle settings\n",
    "    ax.set_xlabel(\"First Digit\", fontsize=14)\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=14)\n",
    "    ax.legend(fontsize=14)\n",
    "    ax.set_xticks(list(range(1, 10)))  # Defining x axis ticks\n",
    "    ax.set_xticklabels(list(range(1, 10)))  # Labels for the ticks\n",
    "\n",
    "    # display the graph\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_finais = np.loadtxt(df_ficheiro, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_real = dados_finais[:, -1]\n",
    "class_real=class_real.astype(int)\n",
    "print(class_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(class_real)\n",
    "print(\"Negativos\", counts[0])\n",
    "print(\"Positivos\", counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafico_digitos (dados_finais,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnosticar_chi_square(first_digit_frequencies, expected_distribution):\n",
    "    \"\"\"\n",
    "    Função para diagnosticar problemas no teste chi-square\n",
    "    \"\"\"\n",
    "    print(\"=== DIAGNÓSTICO CHI-SQUARE ===\")\n",
    "    print(f\"Frequências observadas: {first_digit_frequencies}\")\n",
    "    print(f\"Distribuição esperada: {expected_distribution}\")\n",
    "    print(f\"Soma observadas: {np.sum(first_digit_frequencies)}\")\n",
    "    print(f\"Soma esperadas: {np.sum(expected_distribution)}\")\n",
    "    print(f\"Diferença: {np.sum(first_digit_frequencies) - np.sum(expected_distribution)}\")\n",
    "    print(f\"Diferença relativa: {abs(np.sum(first_digit_frequencies) - np.sum(expected_distribution)) / np.sum(expected_distribution) * 100:.10f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify compliance with the Benford's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_distribution = distribuicao_benford(colunas)\n",
    "\n",
    "previsto_chi2=[]\n",
    "p_values_chi2=[]\n",
    "\n",
    "previsto_f = []\n",
    "previsto_ks=[]\n",
    "previsto_MAD=[]\n",
    "previsto_hel=[]\n",
    "previsto_euc=[]\n",
    "previsto_kl=[]\n",
    "p_values_ks=[]\n",
    "p_values_MAD=[]\n",
    "p_values_euc=[]\n",
    "p_values_hel=[]\n",
    "p_values_kl=[]\n",
    "\n",
    "\n",
    "for n_linha in range(linhas):\n",
    "    dfbl = np.array(dados_finais[n_linha,:-1])\n",
    "    p_values=[]\n",
    "\n",
    "    first_digits = array_primeiros_digitos (dfbl)\n",
    "        \n",
    "    first_digit_frequencies = frequencia_primeiros_digitos(dfbl)\n",
    "\n",
    "    diagnosticar_chi_square(first_digit_frequencies, expected_distribution)\n",
    "    \n",
    "    # Apply the chi-square test\n",
    "    chi2_stat, p_value_chi2 = chisquare(first_digit_frequencies, expected_distribution)\n",
    "    p_value_chi2=1e-15 if p_value_chi2 <(1e-15) else p_value_chi2\n",
    "    predicted_label_chi2 = 1 if p_value_chi2 < alpha else 0  \n",
    "    p_values.append(p_value_chi2)\n",
    "    p_values_chi2.append(p_value_chi2)\n",
    "    previsto_chi2.append(predicted_label_chi2)\n",
    "  \n",
    "    # Apply the absolute mean deviation\n",
    "    MAD_stat, p_value_MAD=teste_hipoteses(\"mad\",first_digit_frequencies,colunas)\n",
    "    p_value_MAD=1e-15 if p_value_MAD <(1e-15) else p_value_MAD\n",
    "    predicted_label_MAD = 1 if p_value_MAD < alpha else 0\n",
    "    p_values.append(p_value_MAD)\n",
    "    p_values_MAD.append(p_value_MAD)\n",
    "    previsto_MAD.append(predicted_label_MAD)\n",
    "    \n",
    "\n",
    "    # Apply to the distance from Kolmogorov-smirnov \n",
    "    ks_stat, p_value_ks=teste_hipoteses(\"ks\",first_digit_frequencies,colunas)\n",
    "    p_value_ks=1e-15 if p_value_ks <(1e-15) else p_value_ks\n",
    "    predicted_label_ks = 1 if p_value_ks < alpha else 0\n",
    "    p_values.append(p_value_ks)\n",
    "    p_values_ks.append(p_value_ks)\n",
    "    previsto_ks.append(predicted_label_ks)\n",
    "\n",
    "    # Apply the euclidean distance\n",
    "    euc_stat, p_value_euc=teste_hipoteses(\"euc\",first_digit_frequencies,colunas)\n",
    "    p_value_euc=1e-15 if p_value_euc <(1e-15) else p_value_euc\n",
    "    predicted_label_euc = 1 if p_value_euc < alpha else 0\n",
    "    p_values.append(p_value_euc)\n",
    "    p_values_euc.append(p_value_euc)\n",
    "    previsto_euc.append(predicted_label_euc)\n",
    "\n",
    "     # Apply the distance of Hellinger\n",
    "    hel_stat, p_value_hel=teste_hipoteses(\"hel\",first_digit_frequencies,colunas)\n",
    "    p_value_hel=1e-15 if p_value_hel <(1e-15) else p_value_hel\n",
    "    predicted_label_hel = 1 if p_value_hel < alpha else 0\n",
    "    p_values.append(p_value_hel)\n",
    "    p_values_hel.append(p_value_hel)\n",
    "    previsto_hel.append(predicted_label_hel)\n",
    "\n",
    "    # Apply the divergence of Kulback-Leibler\n",
    "    kl_stat, p_value_kl=teste_hipoteses(\"kl\",first_digit_frequencies,colunas)\n",
    "    p_value_kl=1e-15 if p_value_kl <(1e-15) else p_value_kl\n",
    "    predicted_label_kl = 1 if p_value_kl < alpha else 0\n",
    "    p_values.append(p_value_kl)\n",
    "    p_values_kl.append(p_value_kl)\n",
    "    previsto_kl.append(predicted_label_kl)\n",
    "\n",
    "\n",
    "    # Fisher’s combination\n",
    "    p_value_fisher = Fisher(p_values) \n",
    "    predicted_label=1 if p_value_fisher<alpha else 0\n",
    "    previsto_f.append(predicted_label)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of metrics\n",
    "metricas = [\"Chi-square\", \"mean absolute deviation\",\"Kolmogorov-Smirnov\", \"Euclidean\", \"Hellinger\", \"Kullback-Leibler\", \"Fisher\"] \n",
    "tns, fps, fns, tps = [], [], [], []\n",
    "\n",
    "\n",
    "previstos = [previsto_chi2, previsto_MAD, previsto_ks, previsto_euc,previsto_hel, previsto_kl, previsto_f]\n",
    "\n",
    "for previsao in previstos:\n",
    "    tn, fp, fn, tp, mc = matriz_confusao(class_real, previsao) \n",
    "    tns.append(tn)\n",
    "    fps.append(fp)\n",
    "    fns.append(fn)\n",
    "    tps.append(tp)\n",
    "\n",
    "# Create DataFrame Pandas\n",
    "df = pd.DataFrame({\n",
    "    \"Metric\": metricas,\n",
    "    \"TN\": tns,\n",
    "    \"FP\": fps,\n",
    "    \"FN\": fns,\n",
    "    \"TP\": tps\n",
    "})\n",
    "\n",
    "# Add evaluation metrics\n",
    "df[\"Precision\"] = df[\"TP\"] / (df[\"TP\"] + df[\"FP\"])\n",
    "df[\"Recall\"] = df[\"TP\"] / (df[\"TP\"] + df[\"FN\"])\n",
    "df[\"F1-score\"] = 2 * (df[\"Precision\"] * df[\"Recall\"]) / (df[\"Precision\"] + df[\"Recall\"])\n",
    "\n",
    "# Display the table\n",
    "print(\"Dataset:\")\n",
    "print(f\"Number of features:{colunas}\")\n",
    "print(f\"Number of instances:{linhas}\")\n",
    "print(\"  \")\n",
    "print(\"Actual class:\")\n",
    "print(f\"Positives --> with anomalies: {counts[1]}\")\n",
    "print(f\"Negatives --> no anomalies: {counts[0]}\")\n",
    "print(\"  \")\n",
    "print(f\"Proportion of anomalies in an anomalous row: {prop_manipul_linha}\")\n",
    "print(\"-----------\")\n",
    "print(f\"|Alpha={alpha}|\")\n",
    "print(\"-----------\")\n",
    "print(df)\n",
    "\n",
    "# Save results in Excel format\n",
    "df.to_excel(f\"Performance.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_values = p_values_chi2\n",
    "alpha_values = np.arange(0, 1, 0.001) \n",
    "p_values = np.array([p_values])\n",
    "\n",
    "# statistics\n",
    "print(\"Statistics of p_values:\")\n",
    "print(\"Average:\", np.mean(p_values))\n",
    "print(\"Standard Deviation:\", np.std(p_values))\n",
    "print(\"Min:\", np.min(p_values))\n",
    "print(\"Max:\", np.max(p_values))\n",
    "\n",
    "fprs, tprs, alpha_values = gerar_roc(class_real, p_values, alpha_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprs = np.array(fprs)\n",
    "tprs = np.array(tprs)\n",
    "alpha_values = np.array(alpha_values)\n",
    "\n",
    "# Criterion of Youden\n",
    "youden_index = np.argmax(tprs - fprs)  \n",
    "best_alpha_youden = alpha_values[youden_index]\n",
    "\n",
    "# Point closest to (0,1)\n",
    "distances = np.sqrt((1 - tprs)**2 + fprs**2)\n",
    "best_alpha_distance = alpha_values[np.argmin(distances)]\n",
    "\n",
    "print(f\"Best cut-off point by Youden’s criterion: {best_alpha_youden}\")\n",
    "print(f\"Best cut-off point by the criterion of least distance: {best_alpha_distance}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
